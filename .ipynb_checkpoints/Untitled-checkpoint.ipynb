{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################################################################\n",
    "# Método 1: Probabilidad bayesiana \n",
    "\n",
    "# Consideraciones:\n",
    "Previamente al cálculo se ha considerado que las siguientes variables no juegan un papel claro en este método: Socio_Demo_01 (edad), Socio_Demo_02 (antigüedad), Socio_Demo_04 (sexo) y Socio_Demo_05 (sector). Así que se ha descartado utilizarlas.\n",
    "\n",
    "\n",
    "De esta forma, se ha considerado oportuno clasificar los clientes el banco entre cuatro distintas categorías:\n",
    "\n",
    "- Categoría 1: clientes del 20% con ingresos altos. \n",
    "\n",
    "- Categoría 2: clientes del 20% con ingresos medios o bajos.\n",
    "\n",
    "- Categoría 3: clientes del 80% con ingresos altos.\n",
    "\n",
    "- Categoría 4: clientes del 80% con ingresos medios o bajos. \n",
    "\n",
    "# Método:\n",
    "\n",
    "En este modelo simple solamente nos fijamos en el último producto contratado para predecir el siguiente. Así, si un cliente ha comprado un producto B, tenemos que calcular la probabilidad de que después de B compre cualquier de los otros productos, y quedarnos con el que tenga una probabilidad más alta.\n",
    "\n",
    "Así, solo tenemos que contar el número de veces que sale el producto B con el producto A y el número de veces que sale el producto B.\n",
    "\n",
    "$$ P(A | B) = \\frac{P(B | A)P(A)}{P(B)} =  \\frac{\\frac{P(B∩A)}{P(A)}P(A)}{P(B)} = \\frac{P(B∩A)}{P(B)}$$\n",
    "\n",
    "\n",
    "Como hemos clasificado los usuarios en 4 categorías, primero tendremos que generar un vector con los dos últimos productos comprados por cada usuario en cada categoría. Una vez hecho esto, y para cada usuario de test, tendremos que clasificarlo en una de las 4 categorías dependiendo del número de productos que tiene y de los ingresos. Luego, cogeremos el último producto contratado y calcularemos las probabilidades del siguiente producto utilizando la fórmula de Bayes con el dataset de train de la categoría previamente seleccionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código comentado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# IMPORT TRAIN DATA\n",
    "data = pd.read_csv('train2.txt', header = 0, delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How many different products do we have?\n",
    "different_produts = data.groupby('Cod_Prod').Cod_Prod.nunique()\n",
    "\n",
    "#First we will separate our data between 20% and 80% users:\n",
    "data_ID_byProd = data.groupby('ID_Customer').size().sort_values(ascending=False, )\n",
    "IDs_20 = data_ID_byProd.head(np.int(data_ID_byProd.size*0.2+1))\n",
    "IDs_80 = data_ID_byProd.tail(np.int(data_ID_byProd.size*0.8-1))\n",
    "ID20_idx = np.in1d(data.ID_Customer.values, IDs_20.index)\n",
    "data_IDs_20 = data.ix[ID20_idx]\n",
    "ID80_idx = np.in1d(data.ID_Customer.values, IDs_80.index)\n",
    "data_IDs_80 = data.ix[ID80_idx]\n",
    "\n",
    "#Let's create an array with all the different users that we have:\n",
    "data_users = data.groupby('ID_Customer').ID_Customer.nunique()\n",
    "\n",
    "#Now we will divide our data into 4 categories: \n",
    "# 1 - Users that buy a lot of producs (20%) with high income (Socio_Demo_03 > 3)\n",
    "# 2 - Users that buy a lot of products (20%) with midium or low income (Socio_Demo_03 <= 3)\n",
    "# 3 - Users that buy a few products (80%) with high income (Socio_Demo_03 > 3)\n",
    "# 4 - Users that buy a few products (80%) with medium or low income (Socio_Demo_03 <= 3)\n",
    "\n",
    "#The new data will be:\n",
    "\n",
    "# Category 1:\n",
    "users = data_users.sample(10000)\n",
    "data_20_high_income = data_IDs_20[(data_IDs_20.Socio_Demo_03 > 3)]\n",
    "data_20_high_income = data_20_high_income[data_20_high_income.ID_Customer.isin(users.index[:])]\n",
    "unique_users_20_high_income = data_20_high_income.groupby('ID_Customer').ID_Customer.nunique()\n",
    "\n",
    "# Category 2:\n",
    "users = data_users.sample(10000)\n",
    "data_20_low_income = data_IDs_20[(data_IDs_20.Socio_Demo_03 <= 3)]\n",
    "data_20_low_income = data_20_low_income[data_20_low_income.ID_Customer.isin(users.index[:])]\n",
    "unique_users_20_low_income = data_20_low_income.groupby('ID_Customer').ID_Customer.nunique()\n",
    "\n",
    "#Cateogry 3:\n",
    "users = data_users.sample(10000)\n",
    "data_80_high_income = data_IDs_80[(data_IDs_80.Socio_Demo_03 > 3)]\n",
    "data_80_high_income = data_80_high_income[data_80_high_income.ID_Customer.isin(users.index[:])]\n",
    "unique_users_80_high_income = data_80_high_income.groupby('ID_Customer').ID_Customer.nunique()\n",
    "\n",
    "# Category 4:\n",
    "users = data_users.sample(10000)\n",
    "data_80_low_income = data_IDs_80[(data_IDs_80.Socio_Demo_03 <= 3)]\n",
    "data_80_low_income = data_80_low_income[data_80_low_income.ID_Customer.isin(users.index[:])]\n",
    "unique_users_80_low_income = data_80_low_income.groupby('ID_Customer').ID_Customer.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To check how well our model work we will select other random users to test:\n",
    "\n",
    "users = data_users.sample(1000)\n",
    "data_test = data[data.ID_Customer.isin(users.index[:])]\n",
    "unique_users_test = data_test.groupby('ID_Customer').ID_Customer.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_two_products(dataFrame,usr):   \n",
    "    \"\"\"\n",
    "    This function returns the last two products purchased by one user.\n",
    "    \"\"\"\n",
    "    \n",
    "    items = dataFrame[dataFrame.ID_Customer == usr].Cod_Fecha.size\n",
    "    \n",
    "    times = []\n",
    "    index_max = 0\n",
    "    index_product = np.zeros(items)\n",
    "    index_second_max = 0\n",
    "    \n",
    "    for i in range(items):\n",
    "        index_product[i] = dataFrame[dataFrame.ID_Customer == usr].index[i]\n",
    "        \n",
    "        data = dataFrame[dataFrame.ID_Customer == usr].Cod_Fecha[index_product[i]]\n",
    "        new_year, month = data.split('-')\n",
    "        new_time = float(new_year) + (float(month)/12)\n",
    "        times.append(float(new_time))\n",
    "    \n",
    "    #Index of last item purchased:\n",
    "    index_max = times.index(max(times))\n",
    "    times[index_max] = 0\n",
    "    #Index of second last item purchased:\n",
    "    index_second_max = times.index(max(times))\n",
    "    times[index_second_max] = 0\n",
    "    \n",
    "    last_product = dataFrame[dataFrame.ID_Customer == usr].Cod_Prod[index_product[index_max]]\n",
    "    second_last_product = dataFrame[dataFrame.ID_Customer == usr].Cod_Prod[index_product[index_second_max]]\n",
    "\n",
    "    return second_last_product, last_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We will now generate two vectors with the last two products purchased by the forth categories of users that we made before:\n",
    "\n",
    "# Category 1:\n",
    "second_last_products_20_high_income = []\n",
    "last_products_20_high_income = []\n",
    "\n",
    "for i in range(unique_users_20_high_income.size):\n",
    "\n",
    "    second_last_product, last_product = last_two_products(data_20_high_income, unique_users_20_high_income.index[i])\n",
    "\n",
    "    if second_last_product != last_product:\n",
    "        second_last_products_20_high_income.append(second_last_product)\n",
    "        last_products_20_high_income.append(last_product)\n",
    "\n",
    "        \n",
    "# Category 2:\n",
    "second_last_products_20_low_income = []\n",
    "last_products_20_low_income = []\n",
    "\n",
    "for i in range(unique_users_20_low_income.size):\n",
    "\n",
    "    second_last_product, last_product = last_two_products(data_20_low_income, unique_users_20_low_income.index[i])\n",
    "\n",
    "    if second_last_product != last_product:\n",
    "        second_last_products_20_low_income.append(second_last_product)\n",
    "        last_products_20_low_income.append(last_product)\n",
    "\n",
    "        \n",
    "# Category 3:\n",
    "second_last_products_80_high_income = []\n",
    "last_products_80_high_income = []\n",
    "\n",
    "for i in range(unique_users_80_high_income.size):\n",
    "\n",
    "    second_last_product, last_product = last_two_products(data_80_high_income, unique_users_80_high_income.index[i])\n",
    "\n",
    "    if second_last_product != last_product:\n",
    "        second_last_products_80_high_income.append(second_last_product)\n",
    "        last_products_80_high_income.append(last_product)\n",
    "\n",
    "        \n",
    "# Category 4:\n",
    "second_last_products_80_low_income = []\n",
    "last_products_80_low_income = []\n",
    "\n",
    "for i in range(unique_users_80_low_income.size):\n",
    "\n",
    "    second_last_product, last_product = last_two_products(data_80_low_income, unique_users_80_low_income.index[i])\n",
    "\n",
    "    if second_last_product != last_product:\n",
    "        second_last_products_80_low_income.append(second_last_product)\n",
    "        last_products_80_low_income.append(last_product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will now create a function that returns the most likly product that a user will buy afeter buying one product. \n",
    "\n",
    "def next_product(number,different_produts,second_last_products,last_products):\n",
    "    \"\"\"\n",
    "    This function returns the product most likely that a user will buy based on the last product purchased ('number') and\n",
    "    the vectors 'second_last_products' and 'last_products'\n",
    "    \"\"\"\n",
    "    \n",
    "    count_down = 0\n",
    "\n",
    "    for i in range(len(last_products)):\n",
    "        if (last_products[i] == number) or (second_last_products[i] == number):\n",
    "            count_down += 1        \n",
    "            \n",
    "    likelihood = np.zeros(different_produts.size)\n",
    "\n",
    "    for j in range(different_produts.size):\n",
    "\n",
    "        count_up = 0\n",
    "        number_last = different_produts.index[j]\n",
    "        number_second = number\n",
    "\n",
    "        for i in range(len(last_products)):\n",
    "            if (last_products[i] == number_last) and (second_last_products[i] == number_second):\n",
    "                count_up += 1 \n",
    "        try:\n",
    "            likelihood[j] = 100*float(count_up)/float(count_down)\n",
    "        except:\n",
    "            likelihood[j] = 0\n",
    "\n",
    "    index = np.argmax(likelihood)\n",
    "    \n",
    "    return different_produts.index[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now let's test how our model works with the test data created before. \n",
    "\n",
    "#First we will extract the last product and second last product purchased by our test users. For the prediccion we will use\n",
    "# the second last product, and to check how well our model works we will use the last product purchased.\n",
    "\n",
    "product_previous = []\n",
    "product_match = []\n",
    "\n",
    "for i in range(int(unique_users_test.size)):\n",
    "    \n",
    "    second_last_product, last_product = last_two_products(data_test, unique_users_test.index[i])\n",
    "\n",
    "    product_previous.append(second_last_product)\n",
    "    product_match.append(last_product)\n",
    "    \n",
    "#And now we make the prediccion:\n",
    "\n",
    "values = np.zeros(len(product_previous))\n",
    "\n",
    "for i in range(int(unique_users_test.size)):\n",
    "    \n",
    "    # Number of products purchased by the user:\n",
    "    total_number_products = data_test[data_test.ID_Customer == unique_users_test.index[i]].Cod_Prod.size - 1\n",
    "    \n",
    "    # Income of the user:\n",
    "    income = data_test[data_test.ID_Customer == unique_users_test.index[i]].Socio_Demo_03.mean()\n",
    "    \n",
    "    if total_number_products >= 7 :\n",
    "        if income >= 3: \n",
    "            # Category 1:\n",
    "            values[i] =next_product(product_previous[i],different_produts,second_last_products_20_high_income,last_products_20_high_income)\n",
    "        else:\n",
    "            #Category 2:\n",
    "            values[i] =next_product(product_previous[i],different_produts,second_last_products_20_low_income,last_products_20_low_income)\n",
    "    else:\n",
    "        if income >= 3:\n",
    "            # Category 3:\n",
    "            values[i] =next_product(product_previous[i],different_produts,second_last_products_80_high_income,last_products_80_high_income)\n",
    "        else:\n",
    "            # Category 4:\n",
    "            values[i] =next_product(product_previous[i],different_produts,second_last_products_80_low_income,last_products_80_low_income)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_answer = 0\n",
    "\n",
    "for i in range(len(product_match)):\n",
    "       \n",
    "    if product_match[i] == values[i]:\n",
    "        correct_answer += 1\n",
    "\n",
    "score = 100*float(correct_answer) / float(len(product_match))\n",
    "print 'Our model can predict the next product that a user will buy with a score of ', score, '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n",
    "Con un método sencillo, basado en probabilidades bayesianas y haciendo una clasificación previa de usuarios, se ha conseguido unos resultados con aciertos promedio del 27%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################################################\n",
    "# Method 2: Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cod_Prod and number of appearances:\n",
      "Cod_Prod\n",
      "601     661756\n",
      "301     426169\n",
      "201     339686\n",
      "2302    268166\n",
      "9993    230423\n",
      "9991    230423\n",
      "2205    106478\n",
      "2704    101727\n",
      "2601    100163\n",
      "704      91809\n",
      "dtype: int64\n",
      "There are 94 different products\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "\n",
    "# IMPORT THE DATA\n",
    "data = pd.read_csv('train2.txt', header = 0, delimiter='|')\n",
    "# Sort the data by users first, and by date later\n",
    "data.sort_values(['ID_Customer', 'Cod_Fecha'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# DATA EXPLORATION\n",
    "# Array containing all different products:\n",
    "products_vect = data.groupby('Cod_Prod').size().sort_values(ascending=False, ).index\n",
    "print 'Cod_Prod and number of appearances:'\n",
    "print data.groupby('Cod_Prod').size().sort_values(ascending=False, ).head(10)\n",
    "print 'There are {} different products'.format(data.groupby('Cod_Prod').size().sort_values(ascending=False, ).size)\n",
    "#print products_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLIT THE DATA INTO 2 GROUPS:\n",
    "\n",
    "1- 20% of users who purchase MORE products. They generate most of the business for the company and are more likely to buy another product.\n",
    "\n",
    "2- 80% of users who purchase LESS products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SPLIT 20% 80%\n",
    "# Assumption: clients who have purchased more products are bringing more benefits to CAJAMAR\n",
    "data_ID_byProd = data.groupby('ID_Customer').size().sort_values(ascending=False, )\n",
    "IDs_20 = data_ID_byProd.head(np.int(data_ID_byProd.size*0.2+1))\n",
    "IDs_80 = data_ID_byProd.tail(np.int(data_ID_byProd.size*0.8-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20% of clients who purchase MORE products are 135275 clients\n",
      "80% of clients who purchase LESS products are 541095 clients\n",
      "7: is the LIMIT of products that a client has purchased to be included in the 20% of top purchasers\n"
     ]
    }
   ],
   "source": [
    "print '20% of clients who purchase MORE products are {} clients'.format(IDs_20.shape[0])\n",
    "print '80% of clients who purchase LESS products are {} clients'.format(IDs_80.shape[0])\n",
    "print '{}: is the LIMIT of products that a client has purchased to be included in the 20% of top purchasers'.format(IDs_20[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with 20% of USERS WHO PURCHASE MORE PRODUCTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION:\n",
    "    # FOR EVERY USER Create one new variable for each Product with values:\n",
    "    - 0: if not purchased\n",
    "    - 1/inverse_order_of_purchasing: if purchased\n",
    "    \n",
    "    # EXAMPLE:\n",
    "    Assuming there are 5 possible products: A, B, C, D, E, F.\n",
    "    \n",
    "    Customer1 purchased, in this order, products: A(year 2010),C(year 2011),F(year 2012).\n",
    "    \n",
    "    This NEW FEATURE has a field for each product, so has 6 fields (length=94 for Cajamar data).\n",
    "    \n",
    "    Then this new feature will be\n",
    "                A   B   C   D   E   F\n",
    "    feature = [1/3, 0, 1/2, 0,  0, 1/1]\n",
    "    \n",
    "    Because F is the last product he purchased, C the second older and the first was A.\n",
    "    \n",
    "    ****\n",
    "    ##IN THIS WAY, INFORMATION ABOUT WHICH PRODUCTS AND IN WHICH ORDER HAVE BEEN PURCHASED BY A USER IS GATHERED\n",
    "    ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cod_Prod</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_Customer</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A0000008</th>\n",
       "      <td>[2707, 9992, 2602, 506, 1001, 3401, 601, 801, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0000011</th>\n",
       "      <td>[704, 2205, 1011, 301, 601, 9991, 9993, 2302]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0000023</th>\n",
       "      <td>[2302, 704, 2705, 301, 9991, 9993, 201, 2704, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0000024</th>\n",
       "      <td>[2205, 2302, 9991, 9993, 2601, 301, 201, 2704,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0000032</th>\n",
       "      <td>[9992, 2102, 9993, 9991, 201, 301, 2704, 2302,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Cod_Prod\n",
       "ID_Customer                                                   \n",
       "A0000008     [2707, 9992, 2602, 506, 1001, 3401, 601, 801, ...\n",
       "A0000011         [704, 2205, 1011, 301, 601, 9991, 9993, 2302]\n",
       "A0000023     [2302, 704, 2705, 301, 9991, 9993, 201, 2704, ...\n",
       "A0000024     [2205, 2302, 9991, 9993, 2601, 301, 201, 2704,...\n",
       "A0000032     [9992, 2102, 9993, 9991, 201, 301, 2704, 2302,..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID20_idx = np.in1d(data.ID_Customer.values, IDs_20.index)\n",
    "data_IDs_20 = data.ix[ID20_idx]\n",
    "# Obtention of  all the products purchased by user in an array\n",
    "data_IDs_20_PRODS = data_IDs_20[['ID_Customer','Cod_Prod']].groupby('ID_Customer').agg(lambda x: [np.array(x['Cod_Prod'].values)])\n",
    "data_IDs_20_PRODS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20% OF ACTIVE USERS PURCHASE THE 40% OF THE PRODUCTS\n"
     ]
    }
   ],
   "source": [
    "print '20% OF ACTIVE USERS PURCHASE THE '+str(100*data_IDs_20.shape[0]/data.shape[0])+'% OF THE PRODUCTS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SPLIT THE LAST PRODUCT PURCHASED (TARGET) FROM ALL OTHERS (INPUTS)\n",
    "data_IDs_20_PRODS_inputs = data_IDs_20_PRODS['Cod_Prod'].apply(lambda x: x[1:])\n",
    "data_IDs_20_PRODS_tagets = data_IDs_20_PRODS['Cod_Prod'].apply(lambda x: x[0])\n",
    "#print data_IDs_20_PRODS_inputs.head()\n",
    "#print data_IDs_20_PRODS_tagets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function that creates new features from puchased products accordingly with the order of purchasing\n",
    "#  not purchased: 0\n",
    "#  bought: 1/inverse_order_of_purchasing\n",
    "def product_features(ID_prods,products_vect):\n",
    "    products_feature = np.zeros(94)\n",
    "    for i in range(ID_prods.size):\n",
    "        idx = np.where(products_vect==ID_prods[i])\n",
    "        products_feature[idx] = 1.0/(i+1)\n",
    "    return products_feature\n",
    "\n",
    "data_ID_20_Prod_aux = data_IDs_20_PRODS_inputs.apply(lambda x:product_features(x,products_vect))\n",
    "#print data_ID_20_Prod_aux.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING THE TRAINING DATASET\n",
    "\n",
    "Consisis of variables Socio_Demo_01, Socio_Demo_02, Socio_Demo_03, Socio_Demo_05 and \"the 94 features containing information about the purchased products\" for every user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN FEATURES AND THEIR DIMENSION:\n",
      "\n",
      "Index([u'Socio_Demo_01', u'Socio_Demo_02', u'Socio_Demo_03', u'Socio_Demo_05',\n",
      "       u'Product_feat'],\n",
      "      dtype='object')\n",
      "(135275L, 98L)\n",
      "EXAMPLE OF AN ARRAY OF FEATURES FOR A SINGLE USER:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.16666667,  0.1       ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.08333333,  0.09090909,  0.        ,  0.        ,\n",
       "        0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.5       ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.125     ,  0.14285714,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.25      ,  0.        ,  0.        ,  0.        ,  0.2       ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.11111111,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.33333333,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  3.        ,\n",
       "        5.        ,  2.        ,  3.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUILD FEATURES DATASET. WITHOUT GENDER, WITH 4 USER ATTRIBUTES, A FEATURE FOR THE DATE AND 94 FEATURES FOR THE PRODUCTS\n",
    "data_IDs_20_groupedID = data_IDs_20.groupby('ID_Customer').mean()\n",
    "data_IDs_20_groupedID.drop(['Cod_Prod','Socio_Demo_04'], axis=1, inplace=True)\n",
    "data_IDs_20_groupedID['Product_feat'] = data_ID_20_Prod_aux.values\n",
    "features = data_IDs_20_groupedID[[u'Socio_Demo_01', u'Socio_Demo_02', u'Socio_Demo_03', u'Socio_Demo_05']]\n",
    "feature_prod = data_IDs_20_groupedID.Product_feat.values\n",
    "train_features_20 = []\n",
    "for i in range(features.shape[0]):\n",
    "    train_features_20.append(np.concatenate((feature_prod[i],features.values[i,:])))\n",
    "train_features_20 = np.array(train_features_20)\n",
    "print 'TRAIN FEATURES AND THEIR DIMENSION:\\n'\n",
    "print data_IDs_20_groupedID.columns\n",
    "print train_features_20.shape\n",
    "# FINALLY A NUMPY ARRAY WITH ALL THE FEATURES FOR EACH USERS CONCATENATED IS OBTAINED\n",
    "print 'EXAMPLE OF AN ARRAY OF FEATURES FOR A SINGLE USER:'\n",
    "train_features_20[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the TARGETS are set in a numpy array also\n",
    "train_targets_20 = data_IDs_20_PRODS_tagets.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE LEARNING PROCESS\n",
    "\n",
    "- SKLEARN ensemble.RandomForestClassifier\n",
    "- Parameters of classifier OPTIMIZED by SKLEARN model_selection.GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# entrenar sobre una alíquota.\n",
    "# CORRER-HO SOBRE EL TEST2.TXT\n",
    "# REPETIR-HO pel 80%\n",
    "\n",
    "# All datasets (Train and Test) are extracted from train2.txt file\n",
    "X_train_20, X_test_20, y_train_20, y_test_20 = sklearn.model_selection.train_test_split( train_features_20, train_targets_20, test_size=0.2, random_state=42)\n",
    "#model = ensemble.RandomForestClassifier().fit(X_train_20, y_train_20.ravel())\n",
    "#prediction = model.predict(X_test_20)\n",
    "#model.score(X_test_20, y_test_20.ravel())\n",
    "\n",
    "# TRAINING DATA IS TOO BIG, TAKES TOO LONG TO TRAIN MODELS ON IT.\n",
    "# TAKE SOME SAMPLES FROM ALL DATA TO TRAIN\n",
    "idx_20 = np.random.randint(y_train_20.size, size=60000)\n",
    "X_train_20 = X_train_20[idx_20,:]\n",
    "y_train_20 = y_train_20[idx_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER OPTIMIZING PARAMETERS, n_estimators=100 AND criterion=\"gini\" HAVE BEEN CHOSEN\n",
      "{'n_estimators': 90, 'criterion': 'gini'}\n",
      "ACCURACY CV SET: 0.6565125\n",
      "RANDOM FOREST ACCURACY ON TEST SET: 0.43507669562\n",
      "TRAINING RANDOM FOREST TOOK 258.223999977 SECONDS\n"
     ]
    }
   ],
   "source": [
    "#param_grid = {'n_estimators': [50, 75, 100],\n",
    "#             'criterion': ['gini','entropy']}\n",
    "start_RF_20 = time.time()\n",
    "\n",
    "print 'AFTER OPTIMIZING PARAMETERS, n_estimators=100 AND criterion=\"gini\" HAVE BEEN CHOSEN'\n",
    "param_grid_RF = {'n_estimators': [90],\n",
    "             'criterion': ['gini']}\n",
    "RF_opt_20 = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid_RF)\n",
    "RF_opt_20.fit(X_train_20, y_train_20.ravel())\n",
    "prediction_test_20 = RF_opt_20.predict(X_test_20)\n",
    "accuracy_test_20_RF = RF_opt_20.score(X_test_20, y_test_20.ravel())\n",
    "print RF_opt_20.best_params_\n",
    "print 'ACCURACY CV SET: ' + str(RF_opt_20.best_score_)\n",
    "print 'RANDOM FOREST ACCURACY ON TEST SET: {}'.format(accuracy_test_20_RF)\n",
    "\n",
    "end_RF_20 = time.time()\n",
    "print 'TRAINING RANDOM FOREST TOOK {} SECONDS'.format(end_RF_20-start_RF_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS PART IS COMMENTED BECAUSE IT TOOK TOO LONG TO TRAIN A MODEL USING ensemble.GradientBoostingClassifier \n",
      "THESE WERE THE RESULTS: \n",
      " - PARAETERS VALUES: {\"n_estimators\": 50, \"max_depth\": 3}\n",
      " - ACURACY CV SET: 0.45465\n",
      " - GRADIENT BOOSTING ACURACY IN TEST SET: 0.437368323785\n",
      " - TRAINING GRADIENT BOOSTING TOOK 1203.56599998 SECONDS\n",
      "\n",
      " PROBABLY, INCREASING THE VALUE OF PARAMETERS \"n_estimators\" AND \"max_depth\" WOULD IMPROVE THE ACCURACY, BUT ITS COMPUTATIONALLY TOO EXPENSIVE FOR THIS PROJECT\n"
     ]
    }
   ],
   "source": [
    "#start_GB = time.time()\n",
    "#param_grid_GB =  {'n_estimators': [100,150],\n",
    "#                'max_depth':[3,4]}\n",
    "#param_grid_GB =  {'n_estimators': [50],\n",
    "#                'max_depth':[3]}\n",
    "#GB_opt = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid_GB)\n",
    "#GB_opt.fit(X_train_20, y_train_20.ravel())\n",
    "#prediction_test_20 = GB_opt.predict(X_test_20)\n",
    "#accuracy_test_20_GB = GB_opt.score(X_test_20, y_test_20.ravel())\n",
    "#print GB_opt.best_params_\n",
    "#print 'ACCURACY: ' + str(GB_opt.best_score_)\n",
    "#print 'GRADIENT BOOSTING ACCURACY ON TEST SET: {}'.format(accuracy_test_20_GB)\n",
    "\n",
    "#end_GB = time.time()\n",
    "#print 'TRAINING GRADIENT BOOSTING TOOK {} SECONDS'.format(end_GB-start_GB)\n",
    "\n",
    "print 'THIS PART IS COMMENTED BECAUSE IT TOOK TOO LONG TO TRAIN A MODEL USING ensemble.GradientBoostingClassifier \\nTHESE WERE THE RESULTS: '\n",
    "print ' - PARAETERS VALUES: {\"n_estimators\": 50, \"max_depth\": 3}'\n",
    "print ' - ACURACY CV SET: 0.45465'\n",
    "print ' - GRADIENT BOOSTING ACURACY IN TEST SET: 0.437368323785'\n",
    "print ' - TRAINING GRADIENT BOOSTING TOOK 1203.56599998 SECONDS'\n",
    "print '\\n PROBABLY, INCREASING THE VALUE OF PARAMETERS \"n_estimators\" AND \"max_depth\" WOULD IMPROVE THE ACCURACY, BUT IT''S COMPUTATIONALLY TOO EXPENSIVE FOR THIS PROJECT'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACCURACY EXCLUDING THE 10% OF THE MOST COMMON PRODUCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY ON DATA WITHOUT 10% MOST COMMON PRODUCTS\n",
      "Random Forest accuracy: 0.302161471356\n",
      "Gradient Boosting accuracy: 0.314663555037\n"
     ]
    }
   ],
   "source": [
    "# ACCURACY EXCLUDING THE 10% OF THE MOST COMMON PRODUCTS\n",
    "num_prods_to_exclude = int(products_vect.size/10)\n",
    "most_common = products_vect[:num_prods_to_exclude-1]\n",
    "most_common_idx_20 = np.invert(np.in1d(y_test_20, most_common))# indices for most common products\n",
    "y_test_20_NO_common = y_test_20[most_common_idx_20]\n",
    "X_test_20_NO_common = X_test_20[most_common_idx_20,:]\n",
    "\n",
    "print 'ACCURACY ON DATA WITHOUT 10% MOST COMMON PRODUCTS'\n",
    "prediction_test_20_RF_noCommon = RF_opt_20.predict(X_test_20_NO_common)\n",
    "accuracy_test_20_RF_noCommon = RF_opt_20.score(X_test_20_NO_common, y_test_20_NO_common.ravel())\n",
    "print 'Random Forest accuracy: {}'.format(accuracy_test_20_RF_noCommon)\n",
    "\n",
    "#prediction_test_20_GB_noCommon = GB_opt.predict(X_test_20_NO_common)\n",
    "#accuracy_test_20_GB_noCommon = GB_opt.score(X_test_20_NO_common, y_test_20_NO_common.ravel())\n",
    "#print 'Gradient Boosting accuracy: {}'.format(accuracy_test_20_GB_noCommon)\n",
    "print 'Gradient Boosting accuracy: 0.314663555037'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same work for 80% os USERS WHO PURCHASE LESS PRODUCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION PROCESS\n",
    "ID80_idx = np.in1d(data.ID_Customer.values, IDs_80.index)\n",
    "data_IDs_80 = data.ix[ID80_idx]\n",
    "\n",
    "# SINCE THIS DATASET IS TOO BIG, 100000 SAMPLES ARA RANDOMLY TAKEN TO WORK ON\n",
    "data_IDs_80 = data_IDs_80.sample(n=100000)\n",
    "\n",
    "# Obtention of  all the products purchased by user in an array\n",
    "data_IDs_80_PRODS = data_IDs_80[['ID_Customer','Cod_Prod']].groupby('ID_Customer').agg(lambda x: [np.array(x['Cod_Prod'].values)])\n",
    "\n",
    "# SPLIT THE LAST PRODUCT PURCHASED (TARGET) FROM ALL OTHERS (INPUTS)\n",
    "data_IDs_80_PRODS_inputs = data_IDs_80_PRODS['Cod_Prod'].apply(lambda x: x[1:])\n",
    "data_IDs_80_PRODS_tagets = data_IDs_80_PRODS['Cod_Prod'].apply(lambda x: x[0])\n",
    "\n",
    "# APPLY FUNCTION \"product_features\"  that creates new features from puchased products accordingly with the order of purchasing\n",
    "#  not purchased: 0\n",
    "#  bought: 1/inverse_order_of_purchasing\n",
    "data_ID_80_Prod_aux = data_IDs_80_PRODS_inputs.apply(lambda x:product_features(x,products_vect))\n",
    "\n",
    "# BUILD THE TRAINING DATATSET\n",
    "# BUILD FEATURES DATASET. WITHOUT GENDER, WITH 4 USER ATTRIBUTES, A FEATURE FOR THE DATE AND 94 FEATURES FOR THE PRODUCTS\n",
    "data_IDs_80_groupedID = data_IDs_80.groupby('ID_Customer').mean()\n",
    "data_IDs_80_groupedID.drop(['Cod_Prod','Socio_Demo_04'], axis=1, inplace=True)\n",
    "data_IDs_80_groupedID['Product_feat'] = data_ID_80_Prod_aux.values\n",
    "features = data_IDs_80_groupedID[[u'Socio_Demo_01', u'Socio_Demo_02', u'Socio_Demo_03', u'Socio_Demo_05']]\n",
    "feature_prod = data_IDs_80_groupedID.Product_feat.values\n",
    "train_features_80 = []\n",
    "for i in range(features.shape[0]):\n",
    "    train_features_80.append(np.concatenate((feature_prod[i],features.values[i,:])))\n",
    "train_features_80 = np.array(train_features_80)\n",
    "# FINALLY A NUMPY ARRAY WITH ALL THE FEATURES FOR EACH USERS CONCATENATED IS OBTAINED\n",
    "\n",
    "# the TARGETS are set in a numpy array also\n",
    "train_targets_80 = data_IDs_80_PRODS_tagets.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER OPTIMIZING PARAMETERS, n_estimators=40 AND criterion=\"gini\" HAVE BEEN CHOSEN\n",
      "{'n_estimators': 40, 'criterion': 'gini'}\n",
      "ACCURACY CV SET: 0.28296\n",
      "RANDOM FOREST ACCURACY ON TEST SET: 0.263091724063\n",
      "TRAINING RANDOM FOREST TOOK 83.8470001221 SECONDS\n"
     ]
    }
   ],
   "source": [
    "# THE LEARNING PROCESS\n",
    "# All datasets (Train and Test) are extracted from train2.txt file\n",
    "X_train_80, X_test_80, y_train_80, y_test_80 = sklearn.model_selection.train_test_split( train_features_80, train_targets_80, test_size=0.2, random_state=42)\n",
    "\n",
    "# TRAINING DATA IS TOO BIG, TAKES TOO LONG TO TRAIN MODELS ON IT.\n",
    "# TAKE SOME SAMPLES FROM ALL DATA TO TRAIN\n",
    "idx = np.random.randint(y_train_80.size, size=100000)\n",
    "X_train_80 = X_train_80[idx,:]\n",
    "y_train_80 = y_train_80[idx]\n",
    "\n",
    "# TRAINING RANDOM FOREST CLASSIFIER\n",
    "start_RF_80 = time.time()\n",
    "\n",
    "#param_grid_RF = {'n_estimators': [80, 100],\n",
    "#             'criterion': ['gini','entropy']}\n",
    "param_grid_RF = {'n_estimators': [40],\n",
    "             'criterion': ['gini']}\n",
    "print 'AFTER OPTIMIZING PARAMETERS, n_estimators=40 AND criterion=\"gini\" HAVE BEEN CHOSEN'\n",
    "\n",
    "RF_opt_80 = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid_RF)\n",
    "RF_opt_80.fit(X_train_80, y_train_80.ravel())\n",
    "prediction_test_80 = RF_opt_80.predict(X_test_80)\n",
    "accuracy_test_80_RF = RF_opt_80.score(X_test_80, y_test_80.ravel())\n",
    "print RF_opt_80.best_params_\n",
    "print 'ACCURACY CV SET: ' + str(RF_opt_80.best_score_)\n",
    "print 'RANDOM FOREST ACCURACY ON TEST SET: {}'.format(accuracy_test_80_RF)\n",
    "\n",
    "end_RF_80 = time.time()\n",
    "print 'TRAINING RANDOM FOREST TOOK {} SECONDS'.format(end_RF_80-start_RF_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY ON DATA WITHOUT 10% MOST COMMON PRODUCTS\n",
      "Random Forest accuracy: 0.00584349593496\n"
     ]
    }
   ],
   "source": [
    "# ACCURACY EXCLUDING THE 10% OF THE MOST COMMON PRODUCTS\n",
    "num_prods_to_exclude = int(products_vect.size/10)\n",
    "most_common = products_vect[:num_prods_to_exclude-1]\n",
    "most_common_idx_80 = np.invert(np.in1d(y_test_80, most_common))# indices for most common products\n",
    "y_test_80_NO_common = y_test_80[most_common_idx_80]\n",
    "X_test_80_NO_common = X_test_80[most_common_idx_80,:]\n",
    "\n",
    "print 'ACCURACY ON DATA WITHOUT 10% MOST COMMON PRODUCTS'\n",
    "prediction_test_80_RF_noCommon = RF_opt_80.predict(X_test_80_NO_common)\n",
    "accuracy_test_80_RF_noCommon = RF_opt_80.score(X_test_80_NO_common, y_test_80_NO_common.ravel())\n",
    "print 'Random Forest accuracy: {}'.format(accuracy_test_80_RF_noCommon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################################################\n",
    "## APPLYING MODELS TO test2.txt DATASET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT THE DATA\n",
    "data_test2 = pd.read_csv('test2.txt', header = 0, delimiter='|')\n",
    "# Sort the data by users first, and by date later\n",
    "data_test2.sort_values(['ID_Customer', 'Cod_Fecha'], ascending=[True, False], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFYING CLIENTS: DO THEY BELONG TO ACTIVE USERS SEGMENT OR PASSIVE USERS SEGMENT?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      Cod_Prod\n",
      "ID_Customer                                                   \n",
      "B0676372                                                 [601]\n",
      "B0676373                                                 [601]\n",
      "B0676374     [9991, 9992, 1011, 201, 2302, 2601, 2704, 301,...\n",
      "B0676376                            [9993, 707, 201, 704, 601]\n",
      "B0676377                                            [201, 601]\n",
      "258989 users to predict\n"
     ]
    }
   ],
   "source": [
    "# Obtention of  all the products purchased by user in an array\n",
    "data_test2_IDs_PRODS = data_test2[['ID_Customer','Cod_Prod']].groupby('ID_Customer').agg(lambda x: [np.array(x['Cod_Prod'].values)])\n",
    "print data_test2_IDs_PRODS.head()\n",
    "print str(data_test2_IDs_PRODS.shape[0]) + ' users to predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CLASSIFY USERS INTO \"ACTIVE\" OR \"PASSIVE\"\n",
    "products_per_client = data_test2_IDs_PRODS['Cod_Prod'].apply(lambda x: x.size)\n",
    "test2_active_idx = products_per_client.values >= 7\n",
    "test2_passive_idx = np.invert(test2_active_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test2_active, test2_passive = data_test2_IDs_PRODS[test2_active_idx], data_test2_IDs_PRODS[test2_passive_idx]\n",
    "# Convert to series\n",
    "test2_active = test2_active['Cod_Prod']\n",
    "test2_passive = test2_passive['Cod_Prod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CREATE FEATURES FOR EVERY USERS\n",
    "# APPLY FUNCTION \"product_features\"  that creates new features from puchased products accordingly with the order of purchasing\n",
    "#  not purchased: 0\n",
    "#  bought: 1/inverse_order_of_purchasing\n",
    "data_test2_ACTIVE_Prod_aux = test2_active.apply(lambda x:product_features(x,products_vect))\n",
    "data_test2_PASSIVE_Prod_aux = test2_passive.apply(lambda x:product_features(x,products_vect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BUILD FEATURES DATASET. WITHOUT GENDER, WITH 4 USER ATTRIBUTES, A FEATURE FOR THE DATE AND 94 FEATURES FOR THE PRODUCTS\n",
    "ID_ACT_idx = np.in1d(data_test2.ID_Customer.values, test2_active.index)\n",
    "data_IDs_ACT = data_test2.ix[ID_ACT_idx]\n",
    "data_IDs_ACT_groupedID = data_IDs_ACT.groupby('ID_Customer').mean()\n",
    "data_IDs_ACT_groupedID.drop(['Cod_Prod','Socio_Demo_04'], axis=1, inplace=True)\n",
    "data_IDs_ACT_groupedID['Product_feat'] = data_test2_ACTIVE_Prod_aux.values\n",
    "features_ACT = data_IDs_ACT_groupedID[[u'Socio_Demo_01', u'Socio_Demo_02', u'Socio_Demo_03', u'Socio_Demo_05']]\n",
    "feature_prod_ACT = data_IDs_ACT_groupedID.Product_feat.values\n",
    "features_ACT_test2 = []\n",
    "for i in range(features_ACT.shape[0]):\n",
    "    features_ACT_test2.append(np.concatenate((feature_prod_ACT[i],features_ACT.values[i,:])))\n",
    "features_ACT_test2 = np.array(features_ACT_test2)\n",
    "# FINALLY A NUMPY ARRAY WITH ALL THE FEATURES FOR EACH USERS CONCATENATED IS OBTAINED\n",
    "\n",
    "ID_PAS_idx = np.in1d(data_test2.ID_Customer.values, test2_passive.index)\n",
    "data_IDs_PAS = data_test2.ix[ID_PAS_idx]\n",
    "data_IDs_PAS_groupedID = data_IDs_PAS.groupby('ID_Customer').mean()\n",
    "data_IDs_PAS_groupedID.drop(['Cod_Prod','Socio_Demo_04'], axis=1, inplace=True)\n",
    "data_IDs_PAS_groupedID['Product_feat'] = data_test2_PASSIVE_Prod_aux.values\n",
    "features_PAS = data_IDs_PAS_groupedID[[u'Socio_Demo_01', u'Socio_Demo_02', u'Socio_Demo_03', u'Socio_Demo_05']]\n",
    "feature_prod_PAS = data_IDs_PAS_groupedID.Product_feat.values\n",
    "features_PAS_test2 = []\n",
    "for i in range(features_PAS.shape[0]):\n",
    "    features_PAS_test2.append(np.concatenate((feature_prod_PAS[i],features_PAS.values[i,:])))\n",
    "features_PAS_test2 = np.array(features_PAS_test2)\n",
    "# FINALLY A NUMPY ARRAY WITH ALL THE FEATURES FOR EACH USERS CONCATENATED IS OBTAINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RUN PREDICTIVE MODELS ON TEST SAMPLES\n",
    "prediction_test2_ACT = RF_opt_20.predict(features_ACT_test2)\n",
    "prediction_test2_PAS = RF_opt_80.predict(features_PAS_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  DELIVERY DATASET\n",
    "result_ACT = np.concatenate((np.array(test2_active.index).reshape(-1,1), prediction_test2_ACT.reshape(-1,1)), axis=1)\n",
    "result_PAS = np.concatenate((np.array(test2_passive.index).reshape(-1,1), prediction_test2_PAS.reshape(-1,1)), axis=1)\n",
    "dataset_final = np.concatenate((result_ACT,result_PAS))\n",
    "dataset_final = dataset_final[dataset_final[:,0].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SAVE TO .txt\n",
    "DATAFRAME_ENTREGA = pd.DataFrame(data=dataset_final, columns=[\"ID_Customer\",\"Cod_Prod\"])\n",
    "DATAFRAME_ENTREGA.to_csv('Test_Mission.txt',sep='|', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
