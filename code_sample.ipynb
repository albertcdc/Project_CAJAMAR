{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################################################################################\n",
    "# Método 1: Probabilidad bayesiana \n",
    "\n",
    "# Consideraciones:\n",
    "Previamente al cálculo se ha considerado que las siguientes variables no juegan un papel claro en este método: Socio_Demo_01 (edad), Socio_Demo_02 (antigüedad), Socio_Demo_04 (sexo) y Socio_Demo_05 (sector). Así que se ha descartado utilizarlas.\n",
    "\n",
    "\n",
    "De esta forma, se ha considerado oportuno clasificar los clientes el banco entre cuatro distintas categorías:\n",
    "\n",
    "- Categoría 1: clientes del 20% con ingresos altos. \n",
    "\n",
    "- Categoría 2: clientes del 20% con ingresos medios o bajos.\n",
    "\n",
    "- Categoría 3: clientes del 80% con ingresos altos.\n",
    "\n",
    "- Categoría 4: clientes del 80% con ingresos medios o bajos. \n",
    "\n",
    "# Método:\n",
    "\n",
    "En este modelo simple solamente nos fijamos en el último producto contratado para predecir el siguiente. Así, si un cliente ha comprado un producto B, tenemos que calcular la probabilidad de que después de B compre cualquier de los otros productos, y quedarnos con el que tenga una probabilidad más alta.\n",
    "\n",
    "Así, solo tenemos que contar el número de veces que sale el producto B con el producto A y el número de veces que sale el producto B.\n",
    "\n",
    "$$ P(A | B) = \\frac{P(B | A)P(A)}{P(B)} =  \\frac{\\frac{P(B∩A)}{P(A)}P(A)}{P(B)} = \\frac{P(B∩A)}{P(B)}$$\n",
    "\n",
    "\n",
    "Como hemos clasificado los usuarios en 4 categorías, primero tendremos que generar un vector con los dos últimos productos comprados por cada usuario en cada categoría. Una vez hecho esto, y para cada usuario de test, tendremos que clasificarlo en una de las 4 categorías dependiendo del número de productos que tiene y de los ingresos. Luego, cogeremos el último producto contratado y calcularemos las probabilidades del siguiente producto utilizando la fórmula de Bayes con el dataset de train de la categoría previamente seleccionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous results:\n",
    "\n",
    "With a simple statistical method, based on Bayesian probabilities, we got a 27% of accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################################################\n",
    "# Method 2: Supervised Learning, Classificaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cod_Prod and number of appearances:\n",
      "Cod_Prod\n",
      "601     661756\n",
      "301     426169\n",
      "201     339686\n",
      "2302    268166\n",
      "9993    230423\n",
      "9991    230423\n",
      "2205    106478\n",
      "2704    101727\n",
      "2601    100163\n",
      "704      91809\n",
      "dtype: int64\n",
      "There are 94 different products\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "\n",
    "# IMPORT THE DATA\n",
    "data = pd.read_csv('train2.txt', header = 0, delimiter='|')\n",
    "# Sort the data by users first, and by date later\n",
    "data.sort_values(['ID_Customer', 'Cod_Fecha'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# DATA EXPLORATION\n",
    "# Array containing all different products:\n",
    "products_vect = data.groupby('Cod_Prod').size().sort_values(ascending=False, ).index \n",
    "print 'Cod_Prod and number of appearances:'\n",
    "print data.groupby('Cod_Prod').size().sort_values(ascending=False, ).head(10)\n",
    "print 'There are {} different products'.format(data.groupby('Cod_Prod').size().sort_values(ascending=False, ).size)\n",
    "#print products_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLIT THE USERS INTO 2 GROUPS:\n",
    "\n",
    "1- 20% of users who purchase MORE products. They generate most of the business for the company and are more likely to buy another product.\n",
    "\n",
    "2- 80% of users who purchase LESS products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20% of clients who purchase MORE products are 135275 clients\n",
      "80% of clients who purchase LESS products are 541095 clients\n",
      "7: is the LIMIT of products that a client has purchased to be included in the 20% of top purchasers\n"
     ]
    }
   ],
   "source": [
    "# SPLIT 20% 80%\n",
    "# Assumption: clients who have purchased more products are bringing more benefits to CAJAMAR\n",
    "data_ID_byProd = data.groupby('ID_Customer').size().sort_values(ascending=False, )\n",
    "IDs_20 = data_ID_byProd.head(np.int(data_ID_byProd.size*0.2+1))\n",
    "IDs_80 = data_ID_byProd.tail(np.int(data_ID_byProd.size*0.8-1))\n",
    "print '20% of clients who purchase MORE products are {} clients'.format(IDs_20.shape[0])\n",
    "print '80% of clients who purchase LESS products are {} clients'.format(IDs_80.shape[0])\n",
    "print '{}: is the LIMIT of products that a client has purchased to be included in the 20% of top purchasers'.format(IDs_20[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with 20% of USERS WHO PURCHASE MORE PRODUCTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION:\n",
    "    # FOR EVERY USER Create one new variable for each Product with values:\n",
    "    - 0: if not purchased\n",
    "    - 1/inverse_order_of_purchasing: if purchased\n",
    "    \n",
    "    # EXAMPLE:\n",
    "    Assuming there are 5 possible products: A, B, C, D, E, F.\n",
    "    \n",
    "    Customer1 purchased, in this order, products: A(year 2010),C(year 2011),F(year 2012).\n",
    "    \n",
    "    This NEW FEATURE has a field for each product, so has 6 fields (length=94 for Cajamar data).\n",
    "    \n",
    "    Then this new feature will be\n",
    "                A   B   C   D   E   F\n",
    "    feature = [1/3, 0, 1/2, 0,  0, 1/1]\n",
    "    \n",
    "    Because F is the last product he purchased, C the second older and the first was A.\n",
    "    \n",
    "    ****\n",
    "    ##IN THIS WAY, INFORMATION ABOUT WHICH PRODUCTS AND IN WHICH ORDER HAVE BEEN PURCHASED BY A USER IS GATHERED\n",
    "    ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cod_Prod</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_Customer</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A0000008</th>\n",
       "      <td>[2707, 9992, 2602, 506, 1001, 3401, 601, 801, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0000011</th>\n",
       "      <td>[704, 2205, 1011, 301, 601, 9991, 9993, 2302]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0000023</th>\n",
       "      <td>[2302, 704, 2705, 301, 9991, 9993, 201, 2704, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0000024</th>\n",
       "      <td>[2205, 2302, 9991, 9993, 2601, 301, 201, 2704,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0000032</th>\n",
       "      <td>[9992, 2102, 9993, 9991, 201, 301, 2704, 2302,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Cod_Prod\n",
       "ID_Customer                                                   \n",
       "A0000008     [2707, 9992, 2602, 506, 1001, 3401, 601, 801, ...\n",
       "A0000011         [704, 2205, 1011, 301, 601, 9991, 9993, 2302]\n",
       "A0000023     [2302, 704, 2705, 301, 9991, 9993, 201, 2704, ...\n",
       "A0000024     [2205, 2302, 9991, 9993, 2601, 301, 201, 2704,...\n",
       "A0000032     [9992, 2102, 9993, 9991, 201, 301, 2704, 2302,..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID20_idx = np.in1d(data.ID_Customer.values, IDs_20.index)\n",
    "data_IDs_20 = data.ix[ID20_idx]\n",
    "# Obtention of  all the products purchased by user in an array\n",
    "data_IDs_20_PRODS = data_IDs_20[['ID_Customer','Cod_Prod']].groupby('ID_Customer').agg(lambda x: [np.array(x['Cod_Prod'].values)])\n",
    "data_IDs_20_PRODS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20% OF ACTIVE USERS PURCHASE THE 40% OF THE PRODUCTS\n"
     ]
    }
   ],
   "source": [
    "print '20% OF ACTIVE USERS PURCHASE THE '+str(100*data_IDs_20.shape[0]/data.shape[0])+'% OF THE PRODUCTS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SPLIT THE LAST PRODUCT PURCHASED (TARGET) FROM ALL OTHERS (INPUTS)\n",
    "data_IDs_20_PRODS_inputs = data_IDs_20_PRODS['Cod_Prod'].apply(lambda x: x[1:])\n",
    "data_IDs_20_PRODS_tagets = data_IDs_20_PRODS['Cod_Prod'].apply(lambda x: x[0])\n",
    "#print data_IDs_20_PRODS_inputs.head()\n",
    "#print data_IDs_20_PRODS_tagets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function that creates new features from puchased products accordingly with the order of purchasing\n",
    "#  not purchased: 0\n",
    "#  bought: 1/inverse_order_of_purchasing\n",
    "def product_features(ID_prods,products_vect):\n",
    "    products_feature = np.zeros(94)\n",
    "    for i in range(ID_prods.size):\n",
    "        idx = np.where(products_vect==ID_prods[i])\n",
    "        products_feature[idx] = 1.0/(i+1)\n",
    "    return products_feature\n",
    "\n",
    "data_ID_20_Prod_aux = data_IDs_20_PRODS_inputs.apply(lambda x:product_features(x,products_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2205 1011  301  601 9991 9993 2302]\n",
      "Int64Index([ 601,  301,  201, 2302, 9993, 9991, 2205, 2704, 2601,  704, 1011,\n",
      "            9992, 2301, 2705, 2503, 2501, 2602, 2707, 2701,  706, 1804, 2102,\n",
      "            1020,  801, 2106, 1801, 2201,  707, 3001, 1013, 1001, 2401, 1002,\n",
      "            2702, 3401, 2202, 1802, 1310, 2706, 1309, 1017, 1306,  702,  708,\n",
      "            2206, 1307, 1022, 2204,  503,  103, 1009, 1311, 1401,  102, 2203,\n",
      "            1007, 1304, 1805, 1301, 1021, 1302, 2703,  705, 2103,  506, 1019,\n",
      "            3101, 1012, 1303, 1501, 2105,  101, 1005, 1803, 1010, 1806, 1006,\n",
      "             104,  504,  703, 1004, 2801, 1015, 2502,  804, 1008, 2104, 1305,\n",
      "            1014,  803, 2901, 1308,  502, 1312],\n",
      "           dtype='int64', name=u'Cod_Prod')\n",
      "[ 0.25        0.33333333  0.          0.14285714  0.16666667  0.2         1.\n",
      "  0.          0.          0.          0.5         0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# TESTING IF product_features WORKED:\n",
    "print data_IDs_20_PRODS_inputs[1]\n",
    "print products_vect\n",
    "print data_ID_20_Prod_aux[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING THE TRAINING DATASET\n",
    "\n",
    "Consisis of variables Socio_Demo_01, Socio_Demo_02, Socio_Demo_03, Socio_Demo_05 and \"the 94 features containing information about the purchased products\" for every user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN FEATURES AND THEIR DIMENSION:\n",
      "\n",
      "Index([u'Socio_Demo_01', u'Socio_Demo_02', u'Socio_Demo_03', u'Socio_Demo_05',\n",
      "       u'Product_feat'],\n",
      "      dtype='object')\n",
      "(135275L, 98L)\n",
      "EXAMPLE OF AN ARRAY OF FEATURES FOR A SINGLE USER:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.16666667,  0.1       ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.08333333,  0.09090909,  0.        ,  0.        ,\n",
       "        0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.5       ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.125     ,  0.14285714,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.25      ,  0.        ,  0.        ,  0.        ,  0.2       ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.11111111,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.33333333,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  3.        ,\n",
       "        5.        ,  2.        ,  3.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUILD FEATURES DATASET. WITHOUT GENDER, WITH 4 USER ATTRIBUTES, A FEATURE FOR THE DATE AND 94 FEATURES FOR THE PRODUCTS\n",
    "data_IDs_20_groupedID = data_IDs_20.groupby('ID_Customer').mean()\n",
    "data_IDs_20_groupedID.drop(['Cod_Prod','Socio_Demo_04'], axis=1, inplace=True)\n",
    "data_IDs_20_groupedID['Product_feat'] = data_ID_20_Prod_aux.values\n",
    "features = data_IDs_20_groupedID[[u'Socio_Demo_01', u'Socio_Demo_02', u'Socio_Demo_03', u'Socio_Demo_05']]\n",
    "feature_prod = data_IDs_20_groupedID.Product_feat.values\n",
    "train_features_20 = []\n",
    "for i in range(features.shape[0]):\n",
    "    train_features_20.append(np.concatenate((feature_prod[i],features.values[i,:])))\n",
    "train_features_20 = np.array(train_features_20)\n",
    "print 'TRAIN FEATURES AND THEIR DIMENSION:\\n'\n",
    "print data_IDs_20_groupedID.columns\n",
    "print train_features_20.shape\n",
    "# FINALLY A NUMPY ARRAY WITH ALL THE FEATURES FOR EACH USERS CONCATENATED IS OBTAINED\n",
    "print 'EXAMPLE OF AN ARRAY OF FEATURES FOR A SINGLE USER:'\n",
    "train_features_20[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the TARGETS are set in a numpy array also\n",
    "train_targets_20 = data_IDs_20_PRODS_tagets.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE LEARNING PROCESS\n",
    "\n",
    "- SKLEARN ensemble.RandomForestClassifier\n",
    "- Parameters of classifier OPTIMIZED by SKLEARN model_selection.GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All datasets (Train and Test) are extracted from train2.txt file\n",
    "X_train_20, X_test_20, y_train_20, y_test_20 = sklearn.model_selection.train_test_split( train_features_20, train_targets_20, test_size=0.2, random_state=42)\n",
    "#model = ensemble.RandomForestClassifier().fit(X_train_20, y_train_20.ravel())\n",
    "#prediction = model.predict(X_test_20)\n",
    "#model.score(X_test_20, y_test_20.ravel())\n",
    "\n",
    "# TRAINING DATA IS TOO BIG, TAKES TOO LONG TO TRAIN MODELS ON IT.\n",
    "# TAKE SOME SAMPLES FROM ALL DATA TO TRAIN\n",
    "idx_20 = np.random.randint(y_train_20.size, size=60000)\n",
    "X_train_20 = X_train_20[idx_20,:]\n",
    "y_train_20 = y_train_20[idx_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER OPTIMIZING PARAMETERS, n_estimators=100 AND criterion=\"gini\" HAVE BEEN CHOSEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 90, 'criterion': 'gini'}\n",
      "ACCURACY CV SET: 0.615183333333\n",
      "RANDOM FOREST ACCURACY ON TEST SET: 0.432008870819\n",
      "TRAINING RANDOM FOREST TOOK 95.4380002022 SECONDS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_RF_20 = time.time()\n",
    "\n",
    "print 'AFTER OPTIMIZING PARAMETERS, n_estimators=100 AND criterion=\"gini\" HAVE BEEN CHOSEN'\n",
    "#param_grid = {'n_estimators': [50, 75, 100],\n",
    "#             'criterion': ['gini','entropy']}\n",
    "param_grid_RF = {'n_estimators': [90],\n",
    "             'criterion': ['gini']}\n",
    "RF_opt_20 = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid_RF)\n",
    "RF_opt_20.fit(X_train_20, y_train_20.ravel())\n",
    "prediction_test_20 = RF_opt_20.predict(X_test_20)\n",
    "accuracy_test_20_RF = RF_opt_20.score(X_test_20, y_test_20.ravel())\n",
    "print RF_opt_20.best_params_\n",
    "print 'ACCURACY CV SET: ' + str(RF_opt_20.best_score_)\n",
    "print 'RANDOM FOREST ACCURACY ON TEST SET: {}'.format(accuracy_test_20_RF)\n",
    "\n",
    "end_RF_20 = time.time()\n",
    "print 'TRAINING RANDOM FOREST TOOK {} SECONDS'.format(end_RF_20-start_RF_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS PART IS COMMENTED BECAUSE IT TOOK TOO LONG TO TRAIN A MODEL USING ensemble.GradientBoostingClassifier \n",
      "THESE WERE THE RESULTS: \n",
      " - PARAETERS VALUES: {\"n_estimators\": 50, \"max_depth\": 3}\n",
      " - ACURACY CV SET: 0.45465\n",
      " - GRADIENT BOOSTING ACURACY IN TEST SET: 0.437368323785\n",
      " - TRAINING GRADIENT BOOSTING TOOK 1203.56599998 SECONDS\n",
      "\n",
      " PROBABLY, INCREASING THE VALUE OF PARAMETERS \"n_estimators\" AND \"max_depth\" WOULD IMPROVE THE ACCURACY, BUT ITS COMPUTATIONALLY TOO EXPENSIVE FOR THIS PROJECT\n"
     ]
    }
   ],
   "source": [
    "#start_GB = time.time()\n",
    "#param_grid_GB =  {'n_estimators': [100,150],\n",
    "#                'max_depth':[3,4]}\n",
    "#param_grid_GB =  {'n_estimators': [50],\n",
    "#                'max_depth':[3]}\n",
    "#GB_opt = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid_GB)\n",
    "#GB_opt.fit(X_train_20, y_train_20.ravel())\n",
    "#prediction_test_20 = GB_opt.predict(X_test_20)\n",
    "#accuracy_test_20_GB = GB_opt.score(X_test_20, y_test_20.ravel())\n",
    "#print GB_opt.best_params_\n",
    "#print 'ACCURACY: ' + str(GB_opt.best_score_)\n",
    "#print 'GRADIENT BOOSTING ACCURACY ON TEST SET: {}'.format(accuracy_test_20_GB)\n",
    "\n",
    "#end_GB = time.time()\n",
    "#print 'TRAINING GRADIENT BOOSTING TOOK {} SECONDS'.format(end_GB-start_GB)\n",
    "\n",
    "print 'THIS PART IS COMMENTED BECAUSE IT TOOK TOO LONG TO TRAIN A MODEL USING ensemble.GradientBoostingClassifier \\nTHESE WERE THE RESULTS: '\n",
    "print ' - PARAETERS VALUES: {\"n_estimators\": 50, \"max_depth\": 3}'\n",
    "print ' - ACURACY CV SET: 0.45465'\n",
    "print ' - GRADIENT BOOSTING ACURACY IN TEST SET: 0.437368323785'\n",
    "print ' - TRAINING GRADIENT BOOSTING TOOK 1203.56599998 SECONDS'\n",
    "print '\\n PROBABLY, INCREASING THE VALUE OF PARAMETERS \"n_estimators\" AND \"max_depth\" WOULD IMPROVE THE ACCURACY, BUT IT''S COMPUTATIONALLY TOO EXPENSIVE FOR THIS PROJECT'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACCURACY EXCLUDING THE 10% OF THE MOST COMMON PRODUCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY ON DATA WITHOUT 10% MOST COMMON PRODUCTS\n",
      "Random Forest accuracy: 0.29843862866\n",
      "Gradient Boosting accuracy: 0.314663555037\n"
     ]
    }
   ],
   "source": [
    "# ACCURACY EXCLUDING THE 10% OF THE MOST COMMON PRODUCTS\n",
    "num_prods_to_exclude = int(products_vect.size/10)\n",
    "most_common = products_vect[:num_prods_to_exclude-1]\n",
    "most_common_idx_20 = np.invert(np.in1d(y_test_20, most_common))# indices for most common products\n",
    "y_test_20_NO_common = y_test_20[most_common_idx_20]\n",
    "X_test_20_NO_common = X_test_20[most_common_idx_20,:]\n",
    "\n",
    "print 'ACCURACY ON DATA WITHOUT 10% MOST COMMON PRODUCTS'\n",
    "prediction_test_20_RF_noCommon = RF_opt_20.predict(X_test_20_NO_common)\n",
    "accuracy_test_20_RF_noCommon = RF_opt_20.score(X_test_20_NO_common, y_test_20_NO_common.ravel())\n",
    "print 'Random Forest accuracy: {}'.format(accuracy_test_20_RF_noCommon)\n",
    "\n",
    "#prediction_test_20_GB_noCommon = GB_opt.predict(X_test_20_NO_common)\n",
    "#accuracy_test_20_GB_noCommon = GB_opt.score(X_test_20_NO_common, y_test_20_NO_common.ravel())\n",
    "#print 'Gradient Boosting accuracy: {}'.format(accuracy_test_20_GB_noCommon)\n",
    "print 'Gradient Boosting accuracy: 0.314663555037'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same work for 80% os USERS WHO PURCHASE LESS PRODUCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION PROCESS\n",
    "ID80_idx = np.in1d(data.ID_Customer.values, IDs_80.index)\n",
    "data_IDs_80 = data.ix[ID80_idx]\n",
    "\n",
    "# Obtention of  all the products purchased by user in an array\n",
    "data_IDs_80_PRODS = data_IDs_80[['ID_Customer','Cod_Prod']].groupby('ID_Customer').agg(lambda x: [np.array(x['Cod_Prod'].values)])\n",
    "\n",
    "# SINCE THIS DATASET IS TOO BIG, 100000 SAMPLES ARA RANDOMLY TAKEN TO WORK ON\n",
    "data_IDs_80_PRODS = data_IDs_80_PRODS.sample(n=100000)\n",
    "\n",
    "# SPLIT THE LAST PRODUCT PURCHASED (TARGET) FROM ALL OTHERS (INPUTS)\n",
    "data_IDs_80_PRODS_inputs = data_IDs_80_PRODS['Cod_Prod'].apply(lambda x: x[1:])\n",
    "data_IDs_80_PRODS_tagets = data_IDs_80_PRODS['Cod_Prod'].apply(lambda x: x[0])\n",
    "\n",
    "# APPLY FUNCTION \"product_features\"  that creates new features from puchased products accordingly with the order of purchasing\n",
    "#  not purchased: 0\n",
    "#  bought: 1/inverse_order_of_purchasing\n",
    "data_ID_80_Prod_aux = data_IDs_80_PRODS_inputs.apply(lambda x:product_features(x,products_vect))\n",
    "\n",
    "# EXTRACT SELECTED CLIENTS\n",
    "idx_selected_clients = np.in1d(data_IDs_80['ID_Customer'], data_IDs_80_PRODS.index)\n",
    "data_IDs_80 = data_IDs_80.ix[idx_selected_clients]\n",
    "\n",
    "# BUILD THE TRAINING DATATSET\n",
    "# BUILD FEATURES DATASET. WITHOUT GENDER, WITH 4 USER ATTRIBUTES, A FEATURE FOR THE DATE AND 94 FEATURES FOR THE PRODUCTS\n",
    "data_IDs_80_groupedID = data_IDs_80.groupby('ID_Customer').mean()\n",
    "data_IDs_80_groupedID.drop(['Cod_Prod','Socio_Demo_04'], axis=1, inplace=True)\n",
    "data_IDs_80_groupedID['Product_feat'] = data_ID_80_Prod_aux.values\n",
    "features = data_IDs_80_groupedID[[u'Socio_Demo_01', u'Socio_Demo_02', u'Socio_Demo_03', u'Socio_Demo_05']]\n",
    "feature_prod = data_IDs_80_groupedID.Product_feat.values\n",
    "train_features_80 = []\n",
    "for i in range(features.shape[0]):\n",
    "    train_features_80.append(np.concatenate((feature_prod[i],features.values[i,:])))\n",
    "train_features_80 = np.array(train_features_80)\n",
    "# FINALLY A NUMPY ARRAY WITH ALL THE FEATURES FOR EACH USERS CONCATENATED IS OBTAINED\n",
    "\n",
    "# the TARGETS are set in a numpy array also\n",
    "train_targets_80 = data_IDs_80_PRODS_tagets.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5       ,  1.        ,  0.        , ...,  5.        ,\n",
       "         2.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ,  0.        , ...,  5.        ,\n",
       "         2.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        , ...,  5.        ,\n",
       "         3.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.33333333,  1.        ,  0.        , ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ,  0.        , ...,  1.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER OPTIMIZING PARAMETERS, n_estimators=40 AND criterion=\"gini\" HAVE BEEN CHOSEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 40, 'criterion': 'gini'}\n",
      "ACCURACY CV SET: 0.68044\n",
      "RANDOM FOREST ACCURACY ON TEST SET: 0.524\n",
      "TRAINING RANDOM FOREST TOOK 55.7460000515 SECONDS\n"
     ]
    }
   ],
   "source": [
    "# THE LEARNING PROCESS\n",
    "# All datasets (Train and Test) are extracted from train2.txt file\n",
    "X_train_80, X_test_80, y_train_80, y_test_80 = sklearn.model_selection.train_test_split( train_features_80, train_targets_80, test_size=0.2, random_state=42)\n",
    "\n",
    "# TRAINING DATA IS TOO BIG, TAKES TOO LONG TO TRAIN MODELS ON IT.\n",
    "# TAKE SOME SAMPLES FROM ALL DATA TO TRAIN\n",
    "idx = np.random.randint(y_train_80.size, size=100000)\n",
    "X_train_80 = X_train_80[idx,:]\n",
    "y_train_80 = y_train_80[idx]\n",
    "\n",
    "# TRAINING RANDOM FOREST CLASSIFIER\n",
    "start_RF_80 = time.time()\n",
    "\n",
    "#param_grid_RF = {'n_estimators': [80, 100],\n",
    "#             'criterion': ['gini','entropy']}\n",
    "param_grid_RF = {'n_estimators': [40],\n",
    "             'criterion': ['gini']}\n",
    "print 'AFTER OPTIMIZING PARAMETERS, n_estimators=40 AND criterion=\"gini\" HAVE BEEN CHOSEN'\n",
    "\n",
    "RF_opt_80 = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid_RF)\n",
    "RF_opt_80.fit(X_train_80, y_train_80.ravel())\n",
    "prediction_test_80 = RF_opt_80.predict(X_test_80)\n",
    "accuracy_test_80_RF = RF_opt_80.score(X_test_80, y_test_80.ravel())\n",
    "print RF_opt_80.best_params_\n",
    "print 'ACCURACY CV SET: ' + str(RF_opt_80.best_score_)\n",
    "print 'RANDOM FOREST ACCURACY ON TEST SET: {}'.format(accuracy_test_80_RF)\n",
    "\n",
    "end_RF_80 = time.time()\n",
    "print 'TRAINING RANDOM FOREST TOOK {} SECONDS'.format(end_RF_80-start_RF_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY ON DATA WITHOUT 10% MOST COMMON PRODUCTS\n",
      "Random Forest accuracy: 0.155645036395\n"
     ]
    }
   ],
   "source": [
    "# ACCURACY EXCLUDING THE 10% OF THE MOST COMMON PRODUCTS\n",
    "num_prods_to_exclude = int(products_vect.size/10)\n",
    "most_common = products_vect[:num_prods_to_exclude-1]\n",
    "most_common_idx_80 = np.invert(np.in1d(y_test_80, most_common))# indices for most common products\n",
    "y_test_80_NO_common = y_test_80[most_common_idx_80]\n",
    "X_test_80_NO_common = X_test_80[most_common_idx_80,:]\n",
    "\n",
    "print 'ACCURACY ON DATA WITHOUT 10% MOST COMMON PRODUCTS'\n",
    "prediction_test_80_RF_noCommon = RF_opt_80.predict(X_test_80_NO_common)\n",
    "accuracy_test_80_RF_noCommon = RF_opt_80.score(X_test_80_NO_common, y_test_80_NO_common.ravel())\n",
    "print 'Random Forest accuracy: {}'.format(accuracy_test_80_RF_noCommon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################################################\n",
    "## APPLYING MODELS TO test2.txt DATASET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT THE DATA\n",
    "data_test2 = pd.read_csv('test2.txt', header = 0, delimiter='|')\n",
    "# Sort the data by users first, and by date later\n",
    "data_test2.sort_values(['ID_Customer', 'Cod_Fecha'], ascending=[True, False], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFYING CLIENTS: DO THEY BELONG TO ACTIVE USERS SEGMENT OR PASSIVE USERS SEGMENT?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      Cod_Prod\n",
      "ID_Customer                                                   \n",
      "B0676372                                                 [601]\n",
      "B0676373                                                 [601]\n",
      "B0676374     [9991, 9992, 1011, 201, 2302, 2601, 2704, 301,...\n",
      "B0676376                            [9993, 707, 201, 704, 601]\n",
      "B0676377                                            [201, 601]\n",
      "258989 users to predict\n"
     ]
    }
   ],
   "source": [
    "# Obtention of  all the products purchased by user in an array\n",
    "data_test2_IDs_PRODS = data_test2[['ID_Customer','Cod_Prod']].groupby('ID_Customer').agg(lambda x: [np.array(x['Cod_Prod'].values)])\n",
    "print data_test2_IDs_PRODS.head()\n",
    "print str(data_test2_IDs_PRODS.shape[0]) + ' users to predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CLASSIFY USERS INTO \"ACTIVE\" OR \"PASSIVE\"\n",
    "products_per_client = data_test2_IDs_PRODS['Cod_Prod'].apply(lambda x: x.size)\n",
    "test2_active_idx = products_per_client.values >= 7\n",
    "test2_passive_idx = np.invert(test2_active_idx)\n",
    "test2_active, test2_passive = data_test2_IDs_PRODS[test2_active_idx], data_test2_IDs_PRODS[test2_passive_idx]\n",
    "# Convert to series\n",
    "test2_active = test2_active['Cod_Prod']\n",
    "test2_passive = test2_passive['Cod_Prod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CREATE FEATURES FOR EVERY USERS\n",
    "# APPLY FUNCTION \"product_features\"  that creates new features from puchased products accordingly with the order of purchasing\n",
    "#  not purchased: 0\n",
    "#  bought: 1/inverse_order_of_purchasing\n",
    "data_test2_ACTIVE_Prod_aux = test2_active.apply(lambda x:product_features(x,products_vect))\n",
    "data_test2_PASSIVE_Prod_aux = test2_passive.apply(lambda x:product_features(x,products_vect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BUILD FEATURES DATASET. WITHOUT GENDER, WITH 4 USER ATTRIBUTES, A FEATURE FOR THE DATE AND 94 FEATURES FOR THE PRODUCTS\n",
    "ID_ACT_idx = np.in1d(data_test2.ID_Customer.values, test2_active.index)\n",
    "data_IDs_ACT = data_test2.ix[ID_ACT_idx]\n",
    "data_IDs_ACT_groupedID = data_IDs_ACT.groupby('ID_Customer').mean()\n",
    "data_IDs_ACT_groupedID.drop(['Cod_Prod','Socio_Demo_04'], axis=1, inplace=True)\n",
    "data_IDs_ACT_groupedID['Product_feat'] = data_test2_ACTIVE_Prod_aux.values\n",
    "features_ACT = data_IDs_ACT_groupedID[[u'Socio_Demo_01', u'Socio_Demo_02', u'Socio_Demo_03', u'Socio_Demo_05']]\n",
    "feature_prod_ACT = data_IDs_ACT_groupedID.Product_feat.values\n",
    "features_ACT_test2 = []\n",
    "for i in range(features_ACT.shape[0]):\n",
    "    features_ACT_test2.append(np.concatenate((feature_prod_ACT[i],features_ACT.values[i,:])))\n",
    "features_ACT_test2 = np.array(features_ACT_test2)\n",
    "# FINALLY A NUMPY ARRAY WITH ALL THE FEATURES FOR EACH USERS CONCATENATED IS OBTAINED\n",
    "\n",
    "ID_PAS_idx = np.in1d(data_test2.ID_Customer.values, test2_passive.index)\n",
    "data_IDs_PAS = data_test2.ix[ID_PAS_idx]\n",
    "data_IDs_PAS_groupedID = data_IDs_PAS.groupby('ID_Customer').mean()\n",
    "data_IDs_PAS_groupedID.drop(['Cod_Prod','Socio_Demo_04'], axis=1, inplace=True)\n",
    "data_IDs_PAS_groupedID['Product_feat'] = data_test2_PASSIVE_Prod_aux.values\n",
    "features_PAS = data_IDs_PAS_groupedID[[u'Socio_Demo_01', u'Socio_Demo_02', u'Socio_Demo_03', u'Socio_Demo_05']]\n",
    "feature_prod_PAS = data_IDs_PAS_groupedID.Product_feat.values\n",
    "features_PAS_test2 = []\n",
    "for i in range(features_PAS.shape[0]):\n",
    "    features_PAS_test2.append(np.concatenate((feature_prod_PAS[i],features_PAS.values[i,:])))\n",
    "features_PAS_test2 = np.array(features_PAS_test2)\n",
    "# FINALLY A NUMPY ARRAY WITH ALL THE FEATURES FOR EACH USERS CONCATENATED IS OBTAINED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN PREDICTIVE MODELS ON TEST SAMPLES AND SAVE FILE TO SUBMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# APPLYING MODELS ON TEST SAMPLES\n",
    "prediction_test2_ACT = RF_opt_20.predict(features_ACT_test2)\n",
    "prediction_test2_PAS = RF_opt_80.predict(features_PAS_test2)\n",
    "# DELIVERY DATASET\n",
    "result_ACT = np.concatenate((np.array(test2_active.index).reshape(-1,1), prediction_test2_ACT.reshape(-1,1)), axis=1)\n",
    "result_PAS = np.concatenate((np.array(test2_passive.index).reshape(-1,1), prediction_test2_PAS.reshape(-1,1)), axis=1)\n",
    "dataset_final = np.concatenate((result_ACT,result_PAS))\n",
    "dataset_final = dataset_final[dataset_final[:,0].argsort()]\n",
    "# SAVE TO .txt\n",
    "DATAFRAME_toSubmit = pd.DataFrame(data=dataset_final, columns=[\"ID_Customer\",\"Cod_Prod\"])\n",
    "DATAFRAME_toSubmit.to_csv('Test_Mission.txt',sep='|', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
